#!/usr/bin/env python3
"""Test suite for Runome Analyzer Python bindings"""

import pytest


def test_charfilters():
    """Test CharFilter bindings"""
    import runome
    
    # Test UnicodeNormalizeCharFilter
    filter = runome.UnicodeNormalizeCharFilter()
    assert filter("ï¼°ï½™ï½”ï½ˆï½ï½") == "Python"
    assert filter("ã»") == "å¹³æˆ"  # NFKC normalization
    
    # Test with different forms
    filter_nfc = runome.UnicodeNormalizeCharFilter("NFC")
    filter_nfd = runome.UnicodeNormalizeCharFilter("NFD")
    # NFC and NFD should handle combining characters differently
    text = "ãŒ"  # Hiragana GA (can be decomposed)
    assert len(filter_nfc(text)) <= len(filter_nfd(text))
    
    # Test RegexReplaceCharFilter
    filter = runome.RegexReplaceCharFilter("è›‡ã®ç›®", "janome")
    assert filter("è›‡ã®ç›®ã¯å½¢æ…‹ç´ è§£æå™¨ã§ã™ã€‚") == "janomeã¯å½¢æ…‹ç´ è§£æå™¨ã§ã™ã€‚"
    
    # Test regex with groups
    filter = runome.RegexReplaceCharFilter(r"(\d+)å¹´", r"\1 year")
    assert filter("2024å¹´ã§ã™") == "2024 yearã§ã™"
    
    # Test callable interface
    assert filter.__call__("2024å¹´ã§ã™") == "2024 yearã§ã™"


def test_tokenfilters_basic():
    """Test basic TokenFilter bindings"""
    import runome
    
    tokenizer = runome.Tokenizer()
    tokens = list(tokenizer.tokenize("ãƒ†ã‚¹ãƒˆTEST"))
    
    # Test LowerCaseFilter
    filter = runome.LowerCaseFilter()
    filtered = list(filter(tokens))
    assert all(hasattr(t, 'surface') for t in filtered)
    assert any(t.surface == "test" for t in filtered)
    
    # Test UpperCaseFilter
    filter = runome.UpperCaseFilter()
    filtered = list(filter(tokens))
    assert any(t.surface == "TEST" for t in filtered)
    
    # Test CompoundNounFilter
    tokens = list(tokenizer.tokenize("å½¢æ…‹ç´ è§£æå™¨"))
    filter = runome.CompoundNounFilter()
    filtered = list(filter(tokens))
    # Should combine consecutive nouns
    assert len(filtered) <= len(tokens)


def test_pos_filters():
    """Test POS-based TokenFilters"""
    import runome
    
    tokenizer = runome.Tokenizer()
    tokens = list(tokenizer.tokenize("æ±äº¬é§…ã§é™ã‚Šã‚‹"))
    
    # Test POSKeepFilter
    filter = runome.POSKeepFilter(["åè©"])
    filtered = list(filter(tokens))
    # Should only keep nouns
    assert all("åè©" in t.part_of_speech for t in filtered)
    assert any(t.surface == "æ±äº¬" for t in filtered)
    assert any(t.surface == "é§…" for t in filtered)
    
    # Test POSStopFilter
    filter = runome.POSStopFilter(["åŠ©è©", "å‹•è©"])
    filtered = list(filter(tokens))
    # Should remove particles and verbs
    assert all("åŠ©è©" not in t.part_of_speech for t in filtered)
    assert all("å‹•è©" not in t.part_of_speech for t in filtered)


def test_terminal_filters():
    """Test terminal TokenFilters that change output type"""
    import runome
    
    tokenizer = runome.Tokenizer()
    
    # Test ExtractAttributeFilter
    tokens = list(tokenizer.tokenize("æ±äº¬é§…"))
    filter = runome.ExtractAttributeFilter("surface")
    results = list(filter(tokens))
    assert all(isinstance(r, str) for r in results)
    assert "æ±äº¬" in results
    assert "é§…" in results
    
    # Test with base_form
    filter = runome.ExtractAttributeFilter("base_form")
    results = list(filter(tokens))
    assert all(isinstance(r, str) for r in results)
    
    # Test TokenCountFilter
    tokens = list(tokenizer.tokenize("ã™ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã®ã†ã¡"))
    pos_filter = runome.POSKeepFilter(["åè©"])
    count_filter = runome.TokenCountFilter("surface", sorted=True)
    
    filtered_tokens = list(pos_filter(tokens))
    results = list(count_filter(filtered_tokens))
    
    assert all(isinstance(r, tuple) and len(r) == 2 for r in results)
    assert all(isinstance(r[0], str) and isinstance(r[1], int) for r in results)
    
    # Check that results are sorted by count (descending)
    counts = [r[1] for r in results]
    assert counts == sorted(counts, reverse=True)
    
    # Check specific counts
    count_dict = dict(results)
    assert count_dict.get("ã‚‚ã‚‚", 0) == 2
    assert count_dict.get("ã™ã‚‚ã‚‚", 0) == 1
    assert count_dict.get("ã†ã¡", 0) == 1


def test_analyzer_basic():
    """Test basic Analyzer functionality"""
    import runome
    
    # Test default analyzer
    analyzer = runome.Analyzer()
    results = list(analyzer.analyze("ãƒ†ã‚¹ãƒˆ"))
    assert len(results) > 0
    assert all(hasattr(t, 'surface') for t in results)
    
    # Test with CharFilters
    analyzer = runome.Analyzer(
        char_filters=[
            runome.UnicodeNormalizeCharFilter(),
            runome.RegexReplaceCharFilter("è›‡ã®ç›®", "janome")
        ]
    )
    results = list(analyzer.analyze("è›‡ã®ç›®ã¯ï¼°ï½™ï½”ï½ˆï½ï½ã§ã™"))
    surfaces = [t.surface for t in results]
    assert "janome" in surfaces
    assert "Python" in surfaces  # Should be normalized


def test_analyzer_full_pipeline():
    """Test complete Analyzer pipeline"""
    import runome
    
    analyzer = runome.Analyzer(
        char_filters=[
            runome.UnicodeNormalizeCharFilter(),
            runome.RegexReplaceCharFilter("è›‡ã®ç›®", "janome")
        ],
        token_filters=[
            runome.CompoundNounFilter(),
            runome.POSStopFilter(["è¨˜å·", "åŠ©è©"]),
            runome.LowerCaseFilter()
        ]
    )
    
    text = "è›‡ã®ç›®ã¯Pure ï¼°ï½™ï½”ï½ˆï½ï½ãªå½¢æ…‹ç´ è§£æå™¨ã§ã™ã€‚"
    results = list(analyzer.analyze(text))
    
    # Extract surfaces
    surfaces = [t.surface for t in results]
    
    # Check expected tokens
    assert "janome" in surfaces
    assert "pure" in surfaces
    assert "python" in surfaces
    assert "ãª" in surfaces
    assert "å½¢æ…‹ç´ è§£æå™¨" in surfaces
    assert "ã§ã™" in surfaces
    
    # Check that particles were filtered out
    assert "ã¯" not in surfaces  # particle
    assert "ã€‚" not in surfaces  # symbol


def test_analyzer_with_terminal_filter():
    """Test Analyzer with terminal filters"""
    import runome
    
    # Test with ExtractAttributeFilter
    analyzer = runome.Analyzer(
        token_filters=[
            runome.POSKeepFilter(["åè©"]),
            runome.ExtractAttributeFilter("surface")
        ]
    )
    
    results = list(analyzer.analyze("æ±äº¬é§…ã§é™ã‚Šã‚‹"))
    assert all(isinstance(r, str) for r in results)
    assert "æ±äº¬" in results
    assert "é§…" in results
    assert "é™ã‚Šã‚‹" not in results  # verb should be filtered out
    
    # Test with TokenCountFilter
    analyzer = runome.Analyzer(
        token_filters=[
            runome.POSKeepFilter(["åè©"]),
            runome.TokenCountFilter("surface", sorted=True)
        ]
    )
    
    results = list(analyzer.analyze("ã™ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã®ã†ã¡"))
    assert all(isinstance(r, tuple) for r in results)
    
    count_dict = dict(results)
    assert count_dict["ã‚‚ã‚‚"] == 2
    assert count_dict["ã™ã‚‚ã‚‚"] == 1
    assert count_dict["ã†ã¡"] == 1


def test_analyzer_wakati_rejection():
    """Test that Analyzer rejects wakati mode tokenizer"""
    import runome
    
    # Create tokenizer with wakati=True
    wakati_tokenizer = runome.Tokenizer(wakati=True)
    
    # Should raise exception
    with pytest.raises(Exception) as excinfo:
        runome.Analyzer(tokenizer=wakati_tokenizer)
    
    assert "wakati=True" in str(excinfo.value)


def test_custom_tokenizer():
    """Test Analyzer with custom tokenizer settings"""
    import runome
    
    # Create tokenizer with user dictionary
    # Note: This assumes no user dictionary file, but tests the parameter passing
    tokenizer = runome.Tokenizer(max_unknown_length=512)
    
    analyzer = runome.Analyzer(
        tokenizer=tokenizer,
        token_filters=[runome.LowerCaseFilter()]
    )
    
    results = list(analyzer.analyze("TEST"))
    assert any(t.surface == "test" for t in results)


def test_filter_chaining():
    """Test complex filter chaining"""
    import runome
    
    # Create a complex pipeline
    analyzer = runome.Analyzer(
        char_filters=[
            runome.RegexReplaceCharFilter(r"\s+", " "),  # Normalize whitespace
            runome.RegexReplaceCharFilter(r"[ï¼-ï½]", ""),  # Remove full-width symbols
            runome.UnicodeNormalizeCharFilter()
        ],
        token_filters=[
            runome.CompoundNounFilter(),
            runome.POSKeepFilter(["åè©", "å‹•è©", "å½¢å®¹è©"]),
            runome.LowerCaseFilter()
        ]
    )
    
    text = "æ±äº¬ã€€ã€€é§…ã§ã€€ã€€é™ã‚Šã‚‹ï¼ï¼"
    results = list(analyzer.analyze(text))
    
    surfaces = [t.surface for t in results]
    assert "æ±äº¬" in surfaces
    assert "é§…" in surfaces
    assert "é™ã‚Šã‚‹" in surfaces


def test_error_handling():
    """Test error handling in bindings"""
    import runome
    
    # Test invalid regex pattern
    with pytest.raises(Exception):
        runome.RegexReplaceCharFilter("[invalid", "replacement")
    
    # Test invalid normalization form
    with pytest.raises(Exception):
        runome.UnicodeNormalizeCharFilter("INVALID")
    
    # Test invalid attribute name
    with pytest.raises(Exception):
        runome.ExtractAttributeFilter("invalid_attribute")
    
    with pytest.raises(Exception):
        runome.TokenCountFilter("invalid_attribute")


if __name__ == "__main__":
    # Run basic smoke test
    print("Running Analyzer bindings smoke test...")
    
    test_charfilters()
    print("âœ“ CharFilters working")
    
    test_tokenfilters_basic()
    print("âœ“ Basic TokenFilters working")
    
    test_pos_filters()
    print("âœ“ POS filters working")
    
    test_terminal_filters()
    print("âœ“ Terminal filters working")
    
    test_analyzer_basic()
    print("âœ“ Basic Analyzer working")
    
    test_analyzer_full_pipeline()
    print("âœ“ Full pipeline working")
    
    test_analyzer_with_terminal_filter()
    print("âœ“ Terminal filter integration working")
    
    test_filter_chaining()
    print("âœ“ Filter chaining working")
    
    print("\nAll tests passed! ğŸ‰")